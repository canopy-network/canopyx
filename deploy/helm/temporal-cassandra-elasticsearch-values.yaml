# Temporal with Cassandra for persistence + Elasticsearch for visibility
# Cassandra: High write throughput for workflow execution (default store)
# Elasticsearch: Required for visibility store (Helm chart limitation)

# Use Cassandra
cassandra:
  enabled: true
  config:
    cluster_size: 1
    max_heap_size: 4G          # Reduced to 4G to fit in 6GB container with off-heap buffers
    heap_new_size: 800M        # 20% of max heap (reduced from 1600M)
    # Optimize for high write throughput
    concurrent_writes: 256      # Doubled from default 128
    concurrent_reads: 128       # Doubled from default 64
  persistence:
    enabled: true
    size: 50Gi
  # NOTE: The Helm incubator Cassandra chart doesn't support persistentVolumeClaimRetentionPolicy
  # PVCs must be manually deleted: kubectl delete pvc -l app.kubernetes.io/instance=temporal

  # Enable Cassandra Prometheus exporter
  exporter:
    enabled: true
    servicemonitor: false  # Disable ServiceMonitor, use pod annotations instead
    port: 5556
  # Add Prometheus annotations for pod discovery
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "5556"
    prometheus.io/path: "/metrics"

# Enable Elasticsearch for visibility (required by Helm chart)
# Minimal configuration: 1 replica, reduced resources
elasticsearch:
  enabled: true
  replicas: 1
  minimumMasterNodes: 1
  persistence:
    enabled: true
    size: 10Gi
  # Use newer ES version with better cgroup v2 support
  imageTag: 7.17.18
  # Fix for cgroup v2 on newer kernels
  esJavaOpts: "-Xms1g -Xmx1g"
  resources:
    requests:
      cpu: "500m"
      memory: "2Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"
  sysctlInitContainer:
    enabled: true
  # Use yellow status for single-node cluster (green requires replicas)
  clusterHealthCheckParams: "wait_for_status=yellow&timeout=1s"

# Disable PostgreSQL
postgresql:
  enabled: false

# Disable MySQL
mysql:
  enabled: false

# Server config
server:
  replicaCount: 3  # Scale to 3 replicas for higher throughput

  # Add Prometheus annotations for all Temporal services (frontend, history, matching, worker)
  # All services expose metrics on port 9090 by default
  frontend:
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9090"
      prometheus.io/path: "/metrics"
      prometheus.io/job: "temporal-frontend"

  history:
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9090"
      prometheus.io/path: "/metrics"
      prometheus.io/job: "temporal-history"

  matching:
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9090"
      prometheus.io/path: "/metrics"
      prometheus.io/job: "temporal-matching"

  worker:
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "9090"
      prometheus.io/path: "/metrics"
      prometheus.io/job: "temporal-worker"

  config:
    numHistoryShards: 512

    # Persistence configuration
    # - Cassandra for default store (workflow execution data)
    # - Elasticsearch for visibility (auto-configured by Helm chart when enabled)
    persistence:
      default:
        driver: "cassandra"
        cassandra:
          hosts: ["temporal-cassandra"]
          port: 9042
          keyspace: temporal
          user: "user"
          password: "password"
          replicationFactor: 1
          consistency:
            default:
              consistency: "local_quorum"
              serialConsistency: "local_serial"

    # Increase rate limits for high-throughput (aggressive scaling)
    # Goal: Find system stability limits, not rate limit bottlenecks
    limits:
      rpc:
        frontend:
          rps: 500000   # 5x increase - test frontend capacity
        history:
          rps: 1000000  # 5x increase - history writes are critical
        matching:
          rps: 500000   # 3.3x increase - PRIMARY bottleneck for scheduling
        worker:
          rps: 200000   # 4x increase - worker task scheduling

      # Per-namespace rate limits (applies to all namespaces)
      # These control workflow execution rate within each namespace
      namespaceStateRPS: 500000     # 5x increase - namespace state changes/sec
      namespaceRPS: 500000          # 5x increase - overall namespace operations/sec
      namespaceActionRPS: 500000    # 5x increase - namespace actions/sec

# Auto-register canopyx namespace on startup
defaultNamespaces:
  - name: canopyx
    retention: 168h  # 7 days
    archivalState:
      history: Disabled
      visibility: Disabled

# Disable monitoring for dev
prometheus:
  enabled: false
grafana:
  enabled: false

# Dynamic config for runtime behavior
# These settings control task queue rate limiting to prevent overload
dynamicConfig:
  # Rate limit task queues to prevent ClickHouse connection exhaustion
  # Historical indexing spawns many workflows; without limits, all run concurrently
  matching.taskQueueLimitPerBuildIdRatePerSecond:
    # Historical indexing - slow and steady (5 blocks/sec = 18k blocks/hour)
    # Regex matches any chain:*:index:historical queue
    - value: 5
      constraints:
        taskQueueName: "{{regex:chain:[0-9]+:index:historical}}"
    # Live indexing - faster to keep up with chain tip
    - value: 10
      constraints:
        taskQueueName: "{{regex:chain:[0-9]+:index:live}}"
    # Ops queues - moderate rate
    - value: 15
      constraints:
        taskQueueName: "{{regex:chain:[0-9]+:ops}}"
    # Default fallback
    - value: 20
      constraints: {}