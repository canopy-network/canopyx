apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: "canopyx"
  namespace: default
spec:
  configuration:
    # User profiles (performance settings)
    # Sized for 2-3 chains doing parallel initial sync (1k blocks each)
    profiles:
      default/do_not_merge_across_partitions_select_final: "1"
      default/optimize_aggregation_in_order: "1"
      default/max_threads: "8"              # Match CPU limit (8 cores)
      default/max_insert_threads: "4"       # Half of max_threads for INSERTs

      # Async INSERT - batch small inserts together (critical for high-frequency indexing)
      default/async_insert: "1"
      default/wait_for_async_insert: "1"           # MUST wait - otherwise RecordIndexed can't find promoted data
      default/async_insert_max_data_size: "10485760"  # Flush at 10MB
      default/async_insert_busy_timeout_ms: "200"     # Or flush every 200ms
      # Memory settings - tuned for 16GB container limit (development profile)
      # max_memory_usage: 14GB allows room for multiple concurrent queries + OS/caches
      default/max_memory_usage: "15032385536"  # 14GB (leaves 2GB for OS/caches)
      # External aggregation: Spill to disk at 7GB (50% of max_memory_usage)
      # Higher threshold = faster queries, fewer disk spills
      default/max_bytes_before_external_group_by: "7516192768"  # 7GB
      default/max_bytes_before_external_sort: "7516192768"  # 7GB
      # Memory-efficient distributed aggregation
      default/distributed_aggregation_memory_efficient: "1"
      default/aggregation_memory_efficient_merge_threads: "4"  # Match max_insert_threads
      # Distributed DDL timeout - ON CLUSTER operations
      # Default 180s is too long; 30s is reasonable for small clusters
      default/distributed_ddl_task_timeout: "30000"  # 30s for ON CLUSTER DDL operations
      # Enable lightweight DELETE for fast, non-blocking deletes (ClickHouse 23.3+)
      # Replaces slow ALTER TABLE DELETE mutations with instant deletes
      default/allow_experimental_lightweight_delete: "1"

      # NOTE: insert_quorum removed - not needed when using single replica connection
      # and wait_for_async_insert=1. The async insert wait ensures data is committed
      # before INSERT returns, which is sufficient for RecordIndexed to find the block.

    # Users configuration
    users:
      canopyx/password: "canopyx"
      canopyx/networks/ip: "::/0"
      canopyx/profile: "default"

    # ZooKeeper/Keeper configuration for replication
    # Optimized for 20s block time - indexing must complete in 5-10s
    zookeeper:
      nodes:
        - host: keeper-canopyx-keeper
          port: 2181
      session_timeout_ms: 30000     # 30s - Allow time for distributed DDL operations
      operation_timeout_ms: 10000   # 10s - DDL operations need more time than data operations

    # Profile settings (user-level configuration)
    # NOTE: max_connections and max_concurrent_queries are SERVER settings,
    # not profile settings. They must be in configuration.files (see server-limits.xml below).
    settings:
      compression/case/method: "zstd"
      compression/case/level: "3"
      compression/case/min_part_size: "10485760"  # 10MB
      compression/case/min_part_size_ratio: "0.01"

      # Background thread pools - tuned for dev (4 cores)
      # Rule: background_pool_size * ratio must be >= 20 (for mutations)
      background_pool_size: "16"                             # Reduced from 32
      background_merges_mutations_concurrency_ratio: "2"     # Reduced from 4 (16*2=32 >= 20)
      background_fetches_pool_size: "4"                      # Reduce fetch threads
      max_parts_merged_at_once: "100"                        # Was 150

      # Background merge optimization (reduces FINAL query overhead)
      merge_tree/max_bytes_to_merge_at_min_space_in_pool: "1073741824"  # 1GB
      merge_tree/max_replicated_merges_in_queue: "16"
      merge_tree/number_of_free_entries_in_pool_to_lower_max_size_of_merge: "8"

    # Custom configuration files
    files:
      # Server-level limits (NOT profile settings)
      # These control total connections and queries across ALL users
      # Required for high-throughput indexing with multiple chains
      config.d/server-limits.xml: |
        <clickhouse>
          <!-- Connection pool optimization - Scaled for 100+ chains -->
          <!-- Indexer formula: (live+hist+reindex blocks) × 20 activities × 8 conns + 100 buffer -->
          <!-- Per chain: (2+5+5) × 20 × 8 + 100 = 2,020 connections -->
          <!-- 100 chains × 2,020 = 202,000 + 20,000 buffer (query apps, admin) = 222,000 -->
          <max_connections>250000</max_connections>
          <!-- Concurrent queries: Each INSERT/SELECT counts as 1 query -->
          <!-- Peak: 100 chains × 12 parallel blocks × 50 concurrent activities = 60k queries -->
          <max_concurrent_queries>100000</max_concurrent_queries>
        </clickhouse>

      # HOSTALIAS FIX: Add cluster and namespace macros
      # Note: from_env with default value requires replace="1" attribute
      config.d/macros.xml: |
        <clickhouse>
          <macros>
            <cluster>canopyx</cluster>
            <namespace from_env="KUBERNETES_NAMESPACE" replace="1">default</namespace>
          </macros>
        </clickhouse>

      config.d/distributed-ddl.xml: |
        <clickhouse>
            <!-- Distributed DDL Configuration -->
            <!-- Controls how many ON CLUSTER queries can run simultaneously -->
            <!-- Default is typically 1, increasing to 64 allows parallel DDL execution -->
            <!-- With 8+ chains initializing simultaneously, we need high parallelism -->
            <distributed_ddl>
                <pool_size>64</pool_size>
                <!-- Path in Keeper where DDL tasks are stored -->
                <path>/clickhouse/task_queue/ddl</path>
            </distributed_ddl>
        </clickhouse>

      config.d/replication-defaults.xml: |
        <clickhouse>
            <!-- Default Replication Configuration -->
            <!-- Simplifies CREATE TABLE syntax by providing defaults for all Replicated* engines -->
            <!-- Using {uuid} macro creates unique paths and avoids Keeper GC delays -->
            <default_replica_path>/clickhouse/tables/{uuid}/{shard}</default_replica_path>
            <default_replica_name>{replica}</default_replica_name>
        </clickhouse>

      # HOSTALIAS FIX: interserver_http_host left at default
      # The FQDN remote_servers config is sufficient for DDL coordination
      config.d/interserver-host.xml: |
        <clickhouse>
            <!-- interserver_http_host intentionally left at default -->
        </clickhouse>

      # HOSTALIAS FIX: Override remote_servers with FQDN hostnames
      # The ClickHouse Operator adds HostAliases that map short hostnames to 127.0.0.1
      # This breaks distributed DDL because nodes can't coordinate via localhost
      # Using FQDN hostnames (.svc.cluster.local) bypasses the HostAlias and resolves correctly
      config.d/remote_servers_override.xml: |
        <clickhouse>
          <remote_servers replace="true">
            <canopyx>
              <shard>
                <internal_replication>true</internal_replication>
                <replica>
                  <host>chi-canopyx-canopyx-0-0.default.svc.cluster.local</host>
                  <port>9000</port>
                  <user>canopyx</user>
                  <password>canopyx</password>
                </replica>
                <replica>
                  <host>chi-canopyx-canopyx-0-1.default.svc.cluster.local</host>
                  <port>9000</port>
                  <user>canopyx</user>
                  <password>canopyx</password>
                </replica>
              </shard>
            </canopyx>
          </remote_servers>
        </clickhouse>

      config.d/storage-policy.xml: |
        <clickhouse>
            <!-- Storage Configuration for Tiered Storage (Hot/Warm/Cold) -->
            <storage_configuration>
                <!-- Define storage disks -->
                <disks>
                    <!-- Hot storage: NVMe for recent data (0-30 days) - 1.9TB -->
                    <hot>
                        <path>/var/lib/clickhouse/hot/</path>
                        <keep_free_space_bytes>10737418240</keep_free_space_bytes>
                    </hot>

                    <!-- Warm storage: SSD for medium-term data (30-180 days) - 4TB -->
                    <warm>
                        <path>/var/lib/clickhouse/warm/</path>
                        <keep_free_space_bytes>10737418240</keep_free_space_bytes>
                    </warm>

                    <!-- Cold storage: HDD for long-term data (180+ days) - 20TB -->
                    <cold>
                        <path>/var/lib/clickhouse/cold/</path>
                        <keep_free_space_bytes>10737418240</keep_free_space_bytes>
                    </cold>
                </disks>

                <!-- Define storage policies -->
                <policies>
                    <!-- Tiered storage policy: automatically moves data based on TTL -->
                    <tiered_storage>
                        <volumes>
                            <!-- Hot volume: Recent data on fast storage -->
                            <hot>
                                <disk>hot</disk>
                                <max_data_part_size_bytes>53687091200</max_data_part_size_bytes>
                            </hot>

                            <!-- Warm volume: Medium-term data -->
                            <warm>
                                <disk>warm</disk>
                                <max_data_part_size_bytes>107374182400</max_data_part_size_bytes>
                            </warm>

                            <!-- Cold volume: Long-term data -->
                            <cold>
                                <disk>cold</disk>
                            </cold>
                        </volumes>

                        <!-- Move data when volume is 80% full -->
                        <move_factor>0.2</move_factor>
                    </tiered_storage>

                    <!-- Default policy: everything on hot storage (for non-tiered tables) -->
                    <default>
                        <volumes>
                            <main>
                                <disk>hot</disk>
                            </main>
                        </volumes>
                    </default>
                </policies>
            </storage_configuration>
        </clickhouse>


    # Cluster configuration
    clusters:
      - name: "canopyx"
        layout:
          # RF=2 (Replication Factor 2) - Optimized for performance
          # Both nodes store 100% of data (not 1/2)
          # Provides redundancy with lower replication overhead
          # Note: ClickHouse Keeper still runs 3 instances for Raft consensus
          shardsCount: 1
          replicasCount: 2

  # Pod and volume templates
  defaults:
    templates:
      podTemplate: clickhouse-pod
      # NOTE: We're manually mounting all storage tiers (hot/warm/cold) in the pod template
      # to ensure they use separate physical disks, so we only need the log volume here
      logVolumeClaimTemplate: log-volume

  templates:
    # Pod template
    podTemplates:
      - name: clickhouse-pod
        spec:
          containers:
            - name: clickhouse
              image: clickhouse/clickhouse-server:25.8
              env:
                # HOSTALIAS FIX: Expose namespace for FQDN macro
                - name: KUBERNETES_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              volumeMounts:
                # All 3 storage tiers must be mounted as SEPARATE paths (not subdirectories)
                # to ensure they use different physical disks (NVMe/SSD/HDD)

                # Hot: /var/lib/clickhouse/hot/ (1.9TB NVMe - fast storage)
                - name: hot-volume
                  mountPath: /var/lib/clickhouse/hot

                # Warm: /var/lib/clickhouse/warm/ (4TB SSD - medium storage)
                - name: warm-volume
                  mountPath: /var/lib/clickhouse/warm

                # Cold: /var/lib/clickhouse/cold/ (20TB HDD - cheap storage)
                - name: cold-volume
                  mountPath: /var/lib/clickhouse/cold
              resources:
                # Sized for 2-3 chains doing parallel initial sync
                # 8 CPU cores, 16GB RAM for handling concurrent INSERTs
                requests:
                  memory: "4Gi"
                  cpu: "2000m"
                limits:
                  memory: "16Gi"
                  cpu: "8000m"    # 8 cores to match max_threads=8

    # Volume claim templates for tiered storage
    # Based on infrastructure-plan.md: 25.9TB total per node (1.9TB hot + 4TB warm + 20TB cold)
    # Production sizing for 3-year capacity with RF=3 replication
    volumeClaimTemplates:
      # Hot storage: NVMe for recent data (0-30 days)
      # Infrastructure plan: 2×1.9TB NVMe RAID 1 = 1.9TB usable
      - name: hot-volume
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 1900Gi  # 1.9TB NVMe (fast SSD)

      # Warm storage: SATA SSD for medium-term data (30-180 days)
      # Infrastructure plan: 2×4TB SATA SSD RAID 1 = 4TB usable
      - name: warm-volume
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 4000Gi  # 4TB SSD

      # Cold storage: HDD for long-term data (180+ days)
      # Infrastructure plan: 2×20TB HDD RAID 1 = 20TB usable
      - name: cold-volume
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 20000Gi  # 20TB HDD

      # Log volume
      - name: log-volume
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi  # 10GB for logs
